## ğŸš€ Vision

<div align="center">

### ğŸŒ Make Decentralized ML

<h2>
  <b>Faster</b> âš¡ &nbsp; | &nbsp;
  <b>Fairer</b> âš–ï¸ &nbsp; | &nbsp;
  <b>Smarter</b> ğŸ§  &nbsp; | &nbsp;
  <b>Stronger</b> ğŸ’ª
</h2>

</div>

We believe the future of machine learning is **decentralized** â€”  where intelligence grows collaboratively, **without data ever leaving its source**.  

Our mission is to **accelerate decentralized ML** and make it the **de facto paradigm** for building intelligent, efficient, and privacy-preserving systems.  

No central authority â€” just **collective learning**, where *everyone contributes* â€” safely, privately, and efficiently.

---

## ğŸ§  What Weâ€™re Building

We **push the boundaries of learning and optimization at the edge**, with research to create *scalable, efficient, and multimodal decentralized learning systems.*

### ğŸ” Key Directions

- âš¡ **Federated & Split Learning** â€” frameworks for decentralized collaboration across heterogeneous devices and institutions.  
- ğŸ§© **Foundation Model Fine-Tuning** â€” adapting large-scale (multimodal) Foundation Models (FMs) to decentralized and resource-constrained environments.  
- ğŸ”’ **Privacy-Preserving Mechanisms** â€” integrating differential privacy, encryption, and secure aggregation into multimodal FMs.  
- ğŸ›°ï¸ **Edge & On-Device Intelligence** â€” enabling lightweight, self-improving models that learn directly where data is generated.  
- ğŸ”„ **Decentralized Optimization & Aggregation** â€” redefining how distributed models synchronize, exchange knowledge, and evolve without central coordination.

---

## ğŸ§© Projects

Weâ€™re developing a growing ecosystem of open-source projects â€” spanning **Efficiency**, **Adaptivity**, **Privacy**, and **Edge FMs** â€” to accelerate the future of decentralized intelligence.

| ğŸŒ Project | ğŸ§¾ Description | ğŸ¯ Focus Area | ğŸ›ï¸ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Venue&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |
|:-----------|:---------------|:--------------|:----------|
| **[FedSTAR](https://github.com/FederatedML/FedSTAR)** | Semi-supervised FL with adaptive reliability. | ğŸ§­ Adaptivity | ICASSP&nbsp;2022 |
| **[FedLN](https://arxiv.org/pdf/2208.09378)** | FL under label noise. | ğŸ§­ Adaptivity | NeurIPS&nbsp;2022&nbsp;*Workshop* |
| **[FedCompress](https://arxiv.org/pdf/2401.14211)** | Task-adaptive model compression for efficient FL. | âš¡ Efficiency | ICASSP&nbsp;2024 |
| **[EncCluster](https://arxiv.org/pdf/2406.09152)** | Scalable FM secure aggregation through weight clustering. | ğŸ”’ Privacy | NeurIPS&nbsp;2024&nbsp;*Workshop* |
| **[DeltaMask](https://arxiv.org/pdf/2311.17299)** | Communication-efficient federated FM fine-tuning via masking. | âš¡ Efficiency / ğŸ›°ï¸ Edge FMs | ICML&nbsp;2024&nbsp;*Workshop* |
| **[MPSL](https://github.com/Nousphera/MPSL)** | Multimodal FM fine-tuning via parallel SL. | ğŸ›°ï¸ Edge FMs / âš¡ Efficiency | IJCAI&nbsp;2025&nbsp;*Workshop* |
| **[MaTU](https://arxiv.org/pdf/2502.06376)** | Many-task federated FM fine-tuning via unified task vectors. | ğŸ§­ Adaptivity / ğŸ›°ï¸ Edge FMs | IJCAI&nbsp;2025 |
| **[EFU](https://arxiv.org/pdf/2502.06376)** | Enforcable Federated Unlearning. | ğŸ§­ Privacy | CIKMI&nbsp;2025 |


<div align="center">

âœ¨ *Accelerating the future of decentralized intelligence â€” together.* âœ¨

</div>

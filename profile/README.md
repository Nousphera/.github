## 🚀 Vision

<div align="center">

### 🌐 Make Decentralized ML

<h2>
  <b>Faster</b> ⚡ &nbsp; | &nbsp;
  <b>Fairer</b> ⚖️ &nbsp; | &nbsp;
  <b>Smarter</b> 🧠 &nbsp; | &nbsp;
  <b>Stronger</b> 💪
</h2>

</div>

We believe the future of machine learning is **decentralized** —  where intelligence grows collaboratively, **without data ever leaving its source**.  

Our mission is to **accelerate decentralized ML** and make it the **de facto paradigm** for building intelligent, efficient, and privacy-preserving systems.  

No central authority — just **collective learning**, where *everyone contributes* — safely, privately, and efficiently.

---

## 🧠 What We’re Building

We **push the boundaries of learning and optimization at the edge**, with research to create *scalable, efficient, and multimodal decentralized learning systems.*

### 🔍 Key Directions

- ⚡ **Federated & Split Learning** — frameworks for decentralized collaboration across heterogeneous devices and institutions.  
- 🧩 **Foundation Model Fine-Tuning** — adapting large-scale (multimodal) Foundation Models (FMs) to decentralized and resource-constrained environments.  
- 🔒 **Privacy-Preserving Mechanisms** — integrating differential privacy, encryption, and secure aggregation into multimodal FMs.  
- 🛰️ **Edge & On-Device Intelligence** — enabling lightweight, self-improving models that learn directly where data is generated.  
- 🔄 **Decentralized Optimization & Aggregation** — redefining how distributed models synchronize, exchange knowledge, and evolve without central coordination.

---

## 🧩 Projects

We’re developing a growing ecosystem of open-source projects — spanning **Efficiency**, **Adaptivity**, **Privacy**, and **Edge FMs** — to accelerate the future of decentralized intelligence.

| 🌐 Project | 🧾 Description | 🎯 Focus Area | 🏛️ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Venue&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |
|:-----------|:---------------|:--------------|:----------|
| **[FedSTAR](https://github.com/FederatedML/FedSTAR)** | Semi-supervised FL with adaptive reliability. | 🧭 Adaptivity | ICASSP&nbsp;2022 |
| **[FedLN](https://arxiv.org/pdf/2208.09378)** | FL under label noise. | 🧭 Adaptivity | NeurIPS&nbsp;2022&nbsp;*Workshop* |
| **[FedCompress](https://arxiv.org/pdf/2401.14211)** | Task-adaptive model compression for efficient FL. | ⚡ Efficiency | ICASSP&nbsp;2024 |
| **[EncCluster](https://arxiv.org/pdf/2406.09152)** | Scalable FM secure aggregation through weight clustering. | 🔒 Privacy | NeurIPS&nbsp;2024&nbsp;*Workshop* |
| **[DeltaMask](https://arxiv.org/pdf/2311.17299)** | Communication-efficient federated FM fine-tuning via masking. | ⚡ Efficiency / 🛰️ Edge FMs | ICML&nbsp;2024&nbsp;*Workshop* |
| **[MPSL](https://github.com/Nousphera/MPSL)** | Multimodal FM fine-tuning via parallel SL. | 🛰️ Edge FMs / ⚡ Efficiency | IJCAI&nbsp;2025&nbsp;*Workshop* |
| **[MaTU](https://arxiv.org/pdf/2502.06376)** | Many-task federated FM fine-tuning via unified task vectors. | 🧭 Adaptivity / 🛰️ Edge FMs | IJCAI&nbsp;2025 |
| **[EFU](https://arxiv.org/pdf/2502.06376)** | Enforcable Federated Unlearning. | 🧭 Privacy | CIKMI&nbsp;2025 |


<div align="center">

✨ *Accelerating the future of decentralized intelligence — together.* ✨

</div>
